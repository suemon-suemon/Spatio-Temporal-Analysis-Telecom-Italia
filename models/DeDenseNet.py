from collections import OrderedDict

import torch
import torch.nn as nn
import torch.nn.functional as F

from models.STBase import STBase


class _DenseLayer(nn.Sequential):
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(_DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features))
        self.add_module('relu1', nn.ReLU(inplace=True))
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,
                                           kernel_size=1, stride=1, bias=False))
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))
        self.add_module('relu2', nn.ReLU(inplace=True))
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,
                                           kernel_size=3, stride=1, padding=1,
                                           bias=False))
        self.drop_rate = drop_rate

    def forward(self, input):
        new_features = super(_DenseLayer, self).forward(input)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate,
                                     training=self.training)
        return torch.cat([input, new_features], 1)


class _DenseBlock(nn.Sequential):
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate,
                                bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)


class DenseNetUnit(nn.Sequential):
    def __init__(self, channels, nb_flows, layers=5, growth_rate=12,
                 num_init_features=32, bn_size=4, drop_rate=0.2):
        super(DenseNetUnit, self).__init__()

        if channels > 0:
            self.features = nn.Sequential(OrderedDict([
                ('conv0', nn.Conv2d(channels, num_init_features, kernel_size=3, padding=1)),
                ('norm0', nn.BatchNorm2d(num_init_features)),
                ('relu0', nn.ReLU(inplace=True))
            ]))

            # Dense Block
            num_features = num_init_features
            num_layers = layers
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,
                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock', block)
            num_features = num_features + num_layers * growth_rate

            # Final batch norm
            dim_inner = 256
            self.features.add_module('normlast', nn.BatchNorm2d(num_features))
            self.features.add_module('convlast', nn.Conv2d(num_features, dim_inner, kernel_size=1, padding=0, bias=False))
            self.features.add_module('relulast', nn.ReLU(inplace=True))
            self.features.add_module('normlast2', nn.BatchNorm2d(dim_inner))
            self.features.add_module('convlast2', nn.Conv2d(dim_inner, nb_flows, kernel_size=1, padding=0, bias=False))

            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight.data)
                elif isinstance(m, nn.BatchNorm2d):
                    m.weight.data.fill_(1)
                    m.bias.data.zero_()
                elif isinstance(m, nn.Linear):
                    m.bias.data.zero_()
        else:
            pass

    def forward(self, x):
        out = self.features(x)
        return out


class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean


class DeDenseNet(STBase):
    def __init__(self, 
                 channels: int=12,
                 pred_len: int=1,
                 *,
                layers_s = 4,
                growth_rate_s = 32,
                num_init_features_s = 32,
                bn_size_s = 4,

                layers_t = 4,
                growth_rate_t = 32,
                num_init_features_t = 32,
                bn_size_t = 4,

                drop_rate = 0.3,
                kernel_size = 25,
                 **kwargs):
        super(DeDenseNet, self).__init__(**kwargs)
        self.channels_close = channels
        self.seq_len = self.channels_close
        self.pred_len = pred_len
        self.save_hyperparameters()

        self.decompsition = series_decomp(kernel_size)
        self.seasonal = DenseNetUnit(self.channels_close, 1, layers_s, growth_rate_s, num_init_features_s, bn_size_s, drop_rate)
        self.trend = DenseNetUnit(self.channels_close, 1, layers_t, growth_rate_t, num_init_features_t, bn_size_t, drop_rate)

    def forward(self, x):
        x = x.squeeze(2)
        # seasonal_init = x
        seasonal_init, trend_init = self.decompsition(x.view(x.size(0), x.size(1), -1))
        seasonal_init = seasonal_init.view(x.shape)
        trend_init = trend_init.view(x.shape)
        
        out = 0
        out += self.seasonal(seasonal_init)
        out += self.trend(trend_init)

        return out.unsqueeze(2)